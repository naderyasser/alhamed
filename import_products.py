"""
Ø³ÙƒØ±ÙŠØ¨Øª Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ù…Ù†ØªØ¬Ø§Øª Ù…Ù† Ø±ÙˆØ§Ø¨Ø· Ø®Ø§Ø±Ø¬ÙŠØ© (Ù†Ø³Ø®Ø© Ù…Ø­Ø³Ù‘Ù†Ø©)
======================================================
ÙŠØ¯Ø¹Ù…:
  - Ø±ÙˆØ§Ø¨Ø· share.google (Google Shopping)
  - Ø±ÙˆØ§Ø¨Ø· amzn.eu (Amazon Ø§Ù„Ù…Ø®ØªØµØ±Ø©)
  - Ø±ÙˆØ§Ø¨Ø· Amazon.eg Ø§Ù„Ù…Ø¨Ø§Ø´Ø±Ø©
  - Ù…ÙˆØ§Ù‚Ø¹ Ù…ØµØ±ÙŠØ©: Ø¨ÙŠ.ØªÙƒØŒ Ø±Ù†ÙŠÙ†ØŒ Ø§Ù„ØºØ²Ø§ÙˆÙŠØŒ Ø§Ù„Ø¹Ø±Ø§Ù‚ÙŠØŒ Rush BrushØŒ HapilinØŒ Cairo Sales

Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…:
    python import_products.py
"""

import os
import sys
import re
import json
import time
import requests
from uuid import uuid4
from urllib.parse import urlparse, urljoin
from bs4 import BeautifulSoup

# â”€â”€ Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù…Ø³Ø§Ø±Ø§Øª â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
sys.path.insert(0, BASE_DIR)

from app import app, db, Product, Category, AdditionalImage, AdditionalData, Cart

UPLOAD_FOLDER = os.path.join(BASE_DIR, 'static', 'uploads')
os.makedirs(UPLOAD_FOLDER, exist_ok=True)

HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
    'Accept-Language': 'ar,en-US;q=0.9,en;q=0.8',
}

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ù†ØªØ¬Ø§Øª Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬Ø© Ù…Ù† Ø§Ù„Ø±ÙˆØ§Ø¨Ø·
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

PRODUCT_LINKS = [
    # Amazon.eg
    {"url": "https://share.google/tBSuLDlTxM0StivQp", "source": "Amazon.eg"},
    {"url": "https://share.google/ERX6YVY3628Zg2639", "source": "Amazon.eg"},
    # Ø´Ø±ÙƒØ© Ø§Ù„Ø¹Ø±Ø§Ù‚Ù‰
    {"url": "https://share.google/U1RGgvrZJeqbTqqMl", "source": "Ø´Ø±ÙƒØ© Ø§Ù„Ø¹Ø±Ø§Ù‚Ù‰"},
    # Ø§Ù„ØºØ²Ø§ÙˆÙŠ
    {"url": "https://share.google/mm9PJWKWjm05Qs7ec", "source": "Ø§Ù„ØºØ²Ø§ÙˆÙŠ"},
    {"url": "https://share.google/CTkARvtfeFcfKBLiu", "source": "Ø§Ù„ØºØ²Ø§ÙˆÙŠ"},
    # Cairo Sales Stores
    {"url": "https://share.google/gwpxbsuFCCIhZQeWW", "source": "Cairo Sales"},
    {"url": "https://share.google/qzxWMCYP9sIPfF6iY", "source": "Cairo Sales"},
    {"url": "https://share.google/8veDpdw9H4Hh3DsPT", "source": "Cairo Sales"},
    # Raneen
    {"url": "https://share.google/8dBlGtqqgH2xX8wx8", "source": "Raneen"},
    # Eval El-Torkey
    {"url": "https://share.google/WcbUj4PZPB6fjlCOw", "source": "Eval El-Torkey"},
    {"url": "https://share.google/v9VQQYJ2gkgEHRImS", "source": "Eval El-Torkey"},
    # Amazon - Ø·Ù‚Ù… Ø£ÙˆØ§Ù†ÙŠ
    {"url": "https://amzn.eu/d/0cfF4hjR", "source": "Amazon.eg",
     "fallback_name": "Ø·Ù‚Ù… Ø£ÙˆØ§Ù†ÙŠ Ø·Ù‡ÙŠ Ù…Ù† Ø§Ù„Ø¬Ø±Ø§Ù†ÙŠØª Ù…Ø§Ø³ØªØ± ÙÙ„Ø§ÙˆØ±ØŒ Ù„ÙˆÙ† Ø£Ø³ÙˆØ¯ 11 Ù‚Ø·Ø¹Ø© ØªØ±ÙˆÙØ§Ù„ØŒ Ù…ØµÙ‚ÙˆÙ„"},
    # Amazon.eg
    {"url": "https://share.google/1ui3HQL5PLY8bBCwR", "source": "Amazon.eg"},
    {"url": "https://share.google/YfqApxBefVWvUNLdN", "source": "Amazon.eg"},
    # Raneen
    {"url": "https://share.google/viZTKCnwZY1H2viUZ", "source": "Raneen"},
    # Ø¨ÙŠ.ØªÙƒ
    {"url": "https://share.google/rdqYKouvJxE2404th", "source": "Ø¨ÙŠ.ØªÙƒ"},
    {"url": "https://share.google/aHHE1FwT17YT7hyA9", "source": "Ø¨ÙŠ.ØªÙƒ"},
    {"url": "https://share.google/tOBsj31CLhLByiRKM", "source": "Ø¨ÙŠ.ØªÙƒ"},
    {"url": "https://share.google/5LXxyCohaWBfSfdz4", "source": "Ø¨ÙŠ.ØªÙƒ"},
    # Amazon.eg
    {"url": "https://share.google/dJMA86V7iwfEMHVyC", "source": "Amazon.eg"},
    {"url": "https://share.google/oDNg9ltpPksMPHn1r", "source": "Amazon.eg"},
    # Amazon - Ù…Ø§ÙƒÙŠÙ†Ø§Øª Ø§Ø²Ø§Ù„Ø© Ø´Ø¹Ø±
    {"url": "https://amzn.eu/d/08j95yTn", "source": "Amazon.eg",
     "fallback_name": "Ù…Ø§ÙƒÙŠÙ†Ø© Ø§Ø²Ø§Ù„Ø© Ø§Ù„Ø´Ø¹Ø± Ù„Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¬Ø§Ù ÙˆØ§Ù„Ø±Ø·Ø¨ Ø³ÙŠÙ„Ùƒ Ø§Ø¨ÙŠÙ„ 5-820 Ù…Ù† Ø¨Ø±Ø§ÙˆÙ†"},
    {"url": "https://amzn.eu/d/0bxoFKS3", "source": "Amazon.eg",
     "fallback_name": "Ù…Ø§ÙƒÙŠÙ†Ø© Ø§Ø²Ø§Ù„Ø© Ø§Ù„Ø´Ø¹Ø± Ø³ÙŠÙ„Ùƒ Ø§Ø¨ÙŠÙ„ 5 Ø³ÙŠÙ†Ø³Ùˆ Ø³Ù…Ø§Ø±Øª 5-620 Ù…Ù† Ø¨Ø±Ø§ÙˆÙ†"},
    {"url": "https://share.google/yzVKFgGr0QHXUPDR7", "source": "Amazon.eg"},
    {"url": "https://amzn.eu/d/0eKUuDah", "source": "Amazon.eg",
     "fallback_name": "Ø¢Ù„Ø© Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø´Ø¹Ø± Ø¨Ø±Ø§ÙˆÙ† Ø³ÙŠÙ„Ùƒ Ø§Ø¨ÙŠÙ„ 513-5 Ù„Ù„Ù…Ø¨ØªØ¯Ø¦Ø§Øª"},
    {"url": "https://amzn.eu/d/0esryerg", "source": "Amazon.eg",
     "fallback_name": "Ù…Ø§ÙƒÙŠÙ†Ø© Ø§Ø²Ø§Ù„Ø© Ø§Ù„Ø´Ø¹Ø± Ø³ÙŠÙ„Ùƒ Ø§Ø¨ÙŠÙ„ 9 Ø³ÙŠÙ†Ø³Ùˆ Ø³Ù…Ø§Ø±Øª 9-720 Ù…Ù† Ø¨Ø±Ø§ÙˆÙ†"},
    # Amazon.eg
    {"url": "https://share.google/8Uku7YZ1YdISZIiW3", "source": "Amazon.eg"},
    # Ø¨ÙŠ.ØªÙƒ
    {"url": "https://share.google/DPquu5uLE50XSVPVf", "source": "Ø¨ÙŠ.ØªÙƒ"},
    # Amazon.eg
    {"url": "https://share.google/7PsDyqQtXsqajEiPD", "source": "Amazon.eg"},
    {"url": "https://share.google/5esxwLoTnk3V5HlFt", "source": "Amazon.eg"},
    # Hapilin
    {"url": "https://share.google/cTNGux2XUhQiVsZga", "source": "Hapilin"},
    # Amazon.eg
    {"url": "https://share.google/ITPxbHYrOAiXx1SHR", "source": "Amazon.eg"},
    # Ø¨ÙŠ.ØªÙƒ
    {"url": "https://share.google/aolhzZwJkThtBeU3A", "source": "Ø¨ÙŠ.ØªÙƒ"},
    # Amazon - Ù…Ø§ÙƒÙŠÙ†Ø© Ø¥Ø²Ø§Ù„Ø© Ø´Ø¹Ø±
    {"url": "https://amzn.eu/d/0cvjJvk7", "source": "Amazon.eg",
     "fallback_name": "Ù…Ø§ÙƒÙŠÙ†Ø© Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø´Ø¹Ø± Ø³ÙŠÙ„Ùƒ Ø§ÙƒØ³Ø¨Ø±Øª Ø¨Ø±Ùˆ Ù…ÙˆØ¯ÙŠÙ„ PL3233 Ù…Ù† Ø¨Ø±Ø§ÙˆÙ†"},
    # Amazon.eg
    {"url": "https://share.google/0QuDcchoEnpA3ww0w", "source": "Amazon.eg"},
    # RUSH BRUSH
    {"url": "https://share.google/hrZu82xSUnD3uSRXY", "source": "RUSH BRUSH"},
    # Amazon.eg
    {"url": "https://share.google/sAyl4dMILSheDE9Qm", "source": "Amazon.eg"},
    # RUSH BRUSH
    {"url": "https://share.google/1XAtrCVzQeW5c9mXZ", "source": "RUSH BRUSH"},
    {"url": "https://share.google/r5SZ9iiLb0d0iZsji", "source": "RUSH BRUSH"},
    {"url": "https://share.google/LTNiv78JOo4Q02YJd", "source": "RUSH BRUSH"},
    # Amazon.eg
    {"url": "https://share.google/k0FyBMJPxtCSuhhUB", "source": "Amazon.eg"},
    {"url": "https://share.google/IGudvNoJukWiuV96Q", "source": "Amazon.eg"},
    {"url": "https://share.google/gAkTJhrLijdBwiwOU", "source": "Amazon.eg"},
    {"url": "https://share.google/xGgrUBawajKM117uw", "source": "Amazon.eg"},
    # RUSH BRUSH
    {"url": "https://share.google/GXditokkatFPH3wD0", "source": "RUSH BRUSH"},
]


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  ÙˆØ¸Ø§Ø¦Ù Ù…Ø³Ø§Ø¹Ø¯Ø©
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def download_image(image_url: str) -> str | None:
    """ØªØ­Ù…ÙŠÙ„ ØµÙˆØ±Ø© Ù…Ù† Ø±Ø§Ø¨Ø· ÙˆØ­ÙØ¸Ù‡Ø§ Ù…Ø­Ù„ÙŠÙ‹Ø§"""
    try:
        resp = requests.get(image_url, headers=HEADERS, timeout=15, stream=True)
        resp.raise_for_status()
        ct = resp.headers.get('content-type', '')
        ext = 'jpg'
        if 'png' in ct:
            ext = 'png'
        elif 'webp' in ct:
            ext = 'webp'
        elif 'gif' in ct:
            ext = 'gif'
        filename = f"{uuid4().hex}.{ext}"
        path = os.path.join(UPLOAD_FOLDER, filename)
        with open(path, 'wb') as f:
            for chunk in resp.iter_content(8192):
                f.write(chunk)
        # ØªØ­Ù‚Ù‚ Ø£Ù† Ø§Ù„Ù…Ù„Ù Ù…Ø´ ÙØ§Ø¶ÙŠ
        if os.path.getsize(path) < 500:
            os.remove(path)
            return None
        return filename
    except Exception as e:
        print(f"   âš  ÙØ´Ù„ ØªØ­Ù…ÙŠÙ„ ØµÙˆØ±Ø©: {e}")
        return None


def scrape_amazon(soup, url: str) -> dict:
    """Ø³Ø­Ø¨ Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† ØµÙØ­Ø© Ø£Ù…Ø§Ø²ÙˆÙ†"""
    # Ø§Ù„Ø§Ø³Ù…
    name = None
    tag = soup.select_one('#productTitle')
    if tag:
        name = tag.get_text(strip=True)

    # Ø§Ù„Ø³Ø¹Ø±
    price = None
    for sel in ['span.a-price span.a-offscreen', '.a-price .a-offscreen',
                '#priceblock_ourprice', '#priceblock_dealprice',
                '.a-price-whole', '#corePrice_feature_div .a-offscreen']:
        tag = soup.select_one(sel)
        if tag:
            txt = tag.get_text(strip=True)
            nums = re.findall(r'[\d,]+\.?\d*', txt.replace(',', ''))
            if nums:
                try:
                    price = float(nums[0])
                    break
                except ValueError:
                    pass

    # Ø§Ù„Ø³Ø¹Ø± Ø§Ù„Ø£ØµÙ„ÙŠ (Ù‚Ø¨Ù„ Ø§Ù„Ø®ØµÙ…)
    original_price = None
    for sel in ['.a-text-price span.a-offscreen', '.basisPrice .a-offscreen',
                '#listPrice', '.a-price[data-a-strike] .a-offscreen']:
        tag = soup.select_one(sel)
        if tag:
            txt = tag.get_text(strip=True)
            nums = re.findall(r'[\d,]+\.?\d*', txt.replace(',', ''))
            if nums:
                try:
                    original_price = float(nums[0])
                    break
                except ValueError:
                    pass

    discount = 0.0
    if original_price and price and original_price > price:
        discount = round(((original_price - price) / original_price) * 100, 1)

    # Ø§Ù„ÙˆØµÙ
    description = ''
    desc_tag = soup.select_one('#productDescription')
    if desc_tag:
        description = desc_tag.get_text(strip=True)[:2000]
    if not description:
        bullets = soup.select('#feature-bullets li span.a-list-item')
        if bullets:
            description = ' | '.join(b.get_text(strip=True) for b in bullets[:10])[:2000]

    # Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©
    image_url = None
    img_tag = soup.select_one('#imgTagWrapperId img, #landingImage')
    if img_tag:
        # Try data-a-dynamic-image first (high res)
        dyn = img_tag.get('data-a-dynamic-image')
        if dyn:
            try:
                d = json.loads(dyn)
                if d:
                    image_url = list(d.keys())[-1]  # Ø§Ø®ØªØ§Ø± Ø£ÙƒØ¨Ø± resolution
            except (json.JSONDecodeError, IndexError):
                pass
        if not image_url:
            image_url = img_tag.get('data-old-hires') or img_tag.get('src')

    # ØµÙˆØ± Ø¥Ø¶Ø§ÙÙŠØ©
    additional = []
    thumbs = soup.select('#altImages img, .imageThumbnail img')
    for t in thumbs[:8]:
        src = t.get('src', '')
        if src and 'sprite' not in src and 'play-button' not in src:
            # Ø­ÙˆÙ‘Ù„ thumbnail Ù„ØµÙˆØ±Ø© ÙƒØ¨ÙŠØ±Ø©
            hi_res = re.sub(r'\._[A-Z0-9_,]+_\.', '.', src)
            if hi_res != image_url and hi_res not in additional:
                additional.append(hi_res)

    return {
        'name': name,
        'price': price,
        'discount': discount,
        'description': description,
        'image_url': image_url,
        'additional_images': additional,
    }


def scrape_generic(soup, url: str) -> dict:
    """Ø³Ø­Ø¨ Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† Ø£ÙŠ Ù…ÙˆÙ‚Ø¹ Ø¢Ø®Ø± (Ø¨ÙŠ.ØªÙƒØŒ Ø±Ù†ÙŠÙ†ØŒ Ø§Ù„ØºØ²Ø§ÙˆÙŠØŒ Ø¥Ù„Ø®)"""
    # Ø§Ù„Ø§Ø³Ù…
    name = None
    for sel in ['h1', '[itemprop="name"]', '.product-title', '.product_title',
                '.product-name', '.product__title', 'h1.page-title',
                '.product-info h1', 'h2.product-name']:
        tag = soup.select_one(sel)
        if tag and tag.get_text(strip=True):
            name = tag.get_text(strip=True)[:300]
            break
    if not name:
        og = soup.find('meta', property='og:title')
        if og:
            name = og.get('content', '')[:300]

    # Ø§Ù„Ø³Ø¹Ø±
    price = None
    for sel in ['[itemprop="price"]', '.price ins', '.price .current',
                '.product-price', '.current-price', 'span.price',
                '.special-price .price', '.product-info-price .price',
                '.price-box .price', 'meta[itemprop="price"]']:
        tag = soup.select_one(sel)
        if tag:
            content = tag.get('content') or tag.get_text(strip=True)
            nums = re.findall(r'[\d,]+\.?\d*', content.replace(',', ''))
            if nums:
                try:
                    price = float(nums[0])
                    break
                except ValueError:
                    pass
    if price is None:
        og = soup.find('meta', property='product:price:amount')
        if og:
            try:
                price = float(og.get('content', '0'))
            except ValueError:
                pass

    # Ø§Ù„Ø³Ø¹Ø± Ù‚Ø¨Ù„ Ø§Ù„Ø®ØµÙ…
    original_price = None
    for sel in ['.price del', '.old-price .price', '.was-price',
                'del .woocommerce-Price-amount', '.compare-price',
                '.price-box .old-price .price']:
        tag = soup.select_one(sel)
        if tag:
            content = tag.get('content') or tag.get_text(strip=True)
            nums = re.findall(r'[\d,]+\.?\d*', content.replace(',', ''))
            if nums:
                try:
                    original_price = float(nums[0])
                    break
                except ValueError:
                    pass

    discount = 0.0
    if original_price and price and original_price > price:
        discount = round(((original_price - price) / original_price) * 100, 1)

    # Ø§Ù„ÙˆØµÙ
    description = ''
    for sel in ['[itemprop="description"]', '.product-description',
                '#productDescription', '.description', '.product__description',
                '.product-info-description', '.product-cms-block',
                '.short-description']:
        tag = soup.select_one(sel)
        if tag:
            description = tag.get_text(strip=True)[:2000]
            break
    if not description:
        og = soup.find('meta', property='og:description')
        if og:
            description = og.get('content', '')[:2000]
    if not description:
        meta_desc = soup.find('meta', attrs={'name': 'description'})
        if meta_desc:
            description = meta_desc.get('content', '')[:2000]

    # Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©
    image_url = None
    for sel in ['[itemprop="image"]', '.product-image img', '#main-image',
                '.gallery-image img', 'img.product-image',
                '.product-media img', '.fotorama img',
                '.product-img img', '.product-gallery__image img']:
        tag = soup.select_one(sel)
        if tag:
            image_url = tag.get('src') or tag.get('data-src') or tag.get('data-lazy') or tag.get('data-zoom')
            if image_url:
                image_url = urljoin(url, image_url)
                break
    if not image_url:
        og = soup.find('meta', property='og:image')
        if og:
            image_url = urljoin(url, og.get('content', ''))

    # ØµÙˆØ± Ø¥Ø¶Ø§ÙÙŠØ©
    additional = []
    for sel in ['.product-gallery img', '.thumbnail img', '[data-gallery] img',
                '.product-images img', '.more-views img', '.product-thumbs img']:
        imgs = soup.select(sel)
        if imgs:
            for img in imgs[:10]:
                src = img.get('src') or img.get('data-src') or img.get('data-lazy')
                if src:
                    full = urljoin(url, src)
                    if full != image_url and full not in additional:
                        additional.append(full)
            if additional:
                break

    return {
        'name': name,
        'price': price,
        'discount': discount,
        'description': description,
        'image_url': image_url,
        'additional_images': additional,
    }


def scrape_product(url: str, fallback_name: str = None) -> dict:
    """Ø³Ø­Ø¨ Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù†ØªØ¬ - ÙŠØ¯Ø¹Ù… Ø£Ù…Ø§Ø²ÙˆÙ† + Ù…ÙˆØ§Ù‚Ø¹ Ø£Ø®Ø±Ù‰"""
    try:
        # ØªØ§Ø¨Ø¹ Ø§Ù„Ù€ redirects
        resp = requests.get(url, headers=HEADERS, timeout=20, allow_redirects=True)
        final_url = resp.url
        resp.raise_for_status()
        soup = BeautifulSoup(resp.text, 'html.parser')

        # Ø§Ø®ØªØ± Ø§Ù„Ù€ scraper Ø§Ù„Ù…Ù†Ø§Ø³Ø¨
        if 'amazon' in final_url.lower():
            data = scrape_amazon(soup, final_url)
        else:
            data = scrape_generic(soup, final_url)

        # Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø§Ø³Ù… Ø§Ù„Ø¨Ø¯ÙŠÙ„ Ù„Ùˆ Ù…ÙÙŠØ´ Ø§Ø³Ù…
        if not data['name'] and fallback_name:
            data['name'] = fallback_name

        data['success'] = bool(data['name'])
        data['final_url'] = final_url
        if not data['success']:
            data['error'] = 'Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø§Ø³Ù… Ø§Ù„Ù…Ù†ØªØ¬'

        return data

    except requests.exceptions.Timeout:
        return {'success': False, 'error': 'Ø§Ù†ØªÙ‡Øª Ù…Ù‡Ù„Ø© Ø§Ù„Ø§ØªØµØ§Ù„', 'final_url': url}
    except requests.exceptions.ConnectionError:
        return {'success': False, 'error': 'ÙØ´Ù„ Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ø§Ù„Ù…ÙˆÙ‚Ø¹', 'final_url': url}
    except Exception as e:
        return {'success': False, 'error': str(e), 'final_url': url}


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def clear_old_data():
    """Ù…Ø³Ø­ ÙƒÙ„ Ø§Ù„Ù…Ù†ØªØ¬Ø§Øª ÙˆØ§Ù„Ø£Ù‚Ø³Ø§Ù… ÙˆØ§Ù„ØµÙˆØ± Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø©"""
    with app.app_context():
        Cart.query.delete()
        AdditionalData.query.delete()
        AdditionalImage.query.delete()
        Product.query.delete()
        Category.query.delete()
        db.session.commit()

    # Ù…Ø³Ø­ Ù…Ù„ÙØ§Øª Ø§Ù„ØµÙˆØ±
    for f in os.listdir(UPLOAD_FOLDER):
        fpath = os.path.join(UPLOAD_FOLDER, f)
        if os.path.isfile(fpath):
            try:
                os.remove(fpath)
            except OSError:
                pass

    print("âœ“ ØªÙ… Ù…Ø³Ø­ ÙƒÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø©")


def get_or_create_category(name: str) -> int:
    """Ø¥Ù†Ø´Ø§Ø¡ Ø£Ùˆ Ø¥ÙŠØ¬Ø§Ø¯ Ù‚Ø³Ù…"""
    cat = Category.query.filter_by(name=name).first()
    if not cat:
        cat = Category(name=name, description=f'Ù…Ù†ØªØ¬Ø§Øª {name}')
        db.session.add(cat)
        db.session.flush()
    return cat.id


def import_single_product(item: dict, index: int, total: int) -> bool:
    """Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ù…Ù†ØªØ¬ ÙˆØ§Ø­Ø¯"""
    url = item['url']
    source = item.get('source', 'Ø¹Ø§Ù…')
    fallback = item.get('fallback_name')

    print(f"\n[{index}/{total}] ğŸ”— {url}")
    print(f"   Ø§Ù„Ù…ØµØ¯Ø±: {source}")

    data = scrape_product(url, fallback)

    if not data.get('success'):
        print(f"   âœ— ÙØ´Ù„: {data.get('error', 'Ø®Ø·Ø£ ØºÙŠØ± Ù…Ø¹Ø±ÙˆÙ')}")
        print(f"   â„¹ Ø§Ù„Ø±Ø§Ø¨Ø· Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ: {data.get('final_url', url)}")
        return False

    name = data['name'][:100] if data['name'] else 'Ù…Ù†ØªØ¬ Ø¨Ø¯ÙˆÙ† Ø§Ø³Ù…'
    price = data.get('price') or 0
    discount = data.get('discount') or 0
    description = data.get('description') or 'Ù„Ø§ ÙŠÙˆØ¬Ø¯ ÙˆØµÙ'

    print(f"   âœ“ {name}")
    print(f"   âœ“ Ø§Ù„Ø³Ø¹Ø±: {price} | Ø§Ù„Ø®ØµÙ…: {discount}%")

    # Ø§Ù„Ù‚Ø³Ù…
    cat_id = get_or_create_category(source)

    # Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©
    main_image = 'default.jpg'
    if data.get('image_url'):
        dl = download_image(data['image_url'])
        if dl:
            main_image = dl
            print(f"   âœ“ ØµÙˆØ±Ø©: {dl}")

    # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…Ù†ØªØ¬
    product = Product(
        name=name,
        price=price,
        discount=discount,
        stock=100,
        description=description[:2000],
        image=main_image,
        category_id=cat_id,
    )
    db.session.add(product)
    db.session.flush()

    # ØµÙˆØ± Ø¥Ø¶Ø§ÙÙŠØ©
    for img_url in (data.get('additional_images') or []):
        img_file = download_image(img_url)
        if img_file:
            db.session.add(AdditionalImage(image=img_file, product_id=product.id))

    db.session.commit()
    print(f"   âœ… ØªÙ… (ID: {product.id})")
    return True


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  Ø§Ù„ØªØ´ØºÙŠÙ„
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def main():
    print("=" * 60)
    print("  Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ù…Ù†ØªØ¬Ø§Øª - Ø§Ù„Ø­Ø§Ù…Ø¯")
    print(f"  Ø¹Ø¯Ø¯ Ø§Ù„Ø±ÙˆØ§Ø¨Ø·: {len(PRODUCT_LINKS)}")
    print("=" * 60)

    confirm = input("\nâš   Ø³ÙŠØªÙ… Ù…Ø³Ø­ ÙƒÙ„ Ø§Ù„Ù…Ù†ØªØ¬Ø§Øª Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø©. Ù‡Ù„ ØªØ±ÙŠØ¯ Ø§Ù„Ù…ØªØ§Ø¨Ø¹Ø©ØŸ (y/Ù†Ø¹Ù…): ").strip().lower()
    if confirm not in ('Ù†Ø¹Ù…', 'y', 'yes'):
        print("ØªÙ… Ø§Ù„Ø¥Ù„ØºØ§Ø¡.")
        return

    with app.app_context():
        clear_old_data()

        success = 0
        failed = 0
        failed_urls = []

        for i, item in enumerate(PRODUCT_LINKS, 1):
            try:
                if import_single_product(item, i, len(PRODUCT_LINKS)):
                    success += 1
                else:
                    failed += 1
                    failed_urls.append(item['url'])
            except Exception as e:
                print(f"   âœ— Ø®Ø·Ø£ ØºÙŠØ± Ù…ØªÙˆÙ‚Ø¹: {e}")
                failed += 1
                failed_urls.append(item['url'])
                db.session.rollback()

            # ØªØ£Ø®ÙŠØ± Ø¨Ø³ÙŠØ· Ø¨ÙŠÙ† Ø§Ù„Ø·Ù„Ø¨Ø§Øª Ø¹Ø´Ø§Ù† Ù…Ø§ ÙŠØªÙ… Ø­Ø¸Ø±Ù†Ø§
            time.sleep(1)

    print("\n" + "=" * 60)
    print(f"  âœ… Ù†Ø¬Ø­: {success}")
    print(f"  âœ— ÙØ´Ù„: {failed}")
    print(f"  ğŸ“¦ Ø¥Ø¬Ù…Ø§Ù„ÙŠ: {len(PRODUCT_LINKS)}")
    if failed_urls:
        print("\n  Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø§Ù„ÙØ§Ø´Ù„Ø©:")
        for u in failed_urls:
            print(f"    - {u}")
    print("=" * 60)


if __name__ == '__main__':
    main()
